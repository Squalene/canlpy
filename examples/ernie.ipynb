{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Embedding\n",
    "import tagme\n",
    "from canlpy.helpers.ernie_helpers import load_name_to_QID,load_QID_to_eid, process_sentences\n",
    "\n",
    "from canlpy.core.components.tokenization import BertTokenizer\n",
    "from canlpy.core.models.ernie.model import ErnieForMaskedLM\n",
    "from canlpy.previous_models.knowledge_bert.modeling import BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "#import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "KNOWLEDGE_DIR = '../canlpy/knowledge/ernie/'\n",
    "PRE_TRAINED_DIR = '../canlpy/pretrained_models/ernie/ernie_base/'\n",
    "\n",
    "NAME_TO_QID_FILE = KNOWLEDGE_DIR+ 'entity_map.txt'\n",
    "QID_TO_EID_FILE = KNOWLEDGE_DIR+ 'entity2id.txt'\n",
    "EID_TO_VEC_FILE = PRE_TRAINED_DIR + 'entity2vec.pt'\n",
    "\n",
    "# Set the authorization token for subsequent calls.\n",
    "tagme.GCUBE_TOKEN = \"3aaa1f03-b732-49a1-9328-7f86cdaa13df-843339462\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_original,_ = BertForMaskedLM.from_pretrained(PRE_TRAINED_DIR)\n",
    "model_original.eval()\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model_custom, _ = ErnieForMaskedLM.from_pretrained(PRE_TRAINED_DIR)\n",
    "model_custom.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted token is who\n",
      "The predictions between custom and original are equal: True\n",
      "The predicted token is is\n",
      "The predictions between custom and original are equal: True\n",
      "The predicted token is jim\n",
      "The predictions between custom and original are equal: True\n",
      "The predicted token is henson\n",
      "The predictions between custom and original are equal: True\n",
      "The predicted token is ?\n",
      "The predictions between custom and original are equal: True\n",
      "The predicted token is \"\n",
      "The predictions between custom and original are equal: True\n",
      "The predicted token is jim\n",
      "The predictions between custom and original are equal: True\n",
      "The predicted token is henson\n",
      "The predictions between custom and original are equal: True\n",
      "The predicted token is was\n",
      "The predictions between custom and original are equal: True\n"
     ]
    }
   ],
   "source": [
    "#Suppose to predict hensen for idx 8\n",
    "def eval_sentence(text_a,text_b,model_original,model_custom,tokenizer):\n",
    "    for masked_idx in range(1,10):\n",
    "        tokens_tensor,ents_tensor,ent_mask,segments_tensors = process_sentences(text_a,text_b,[masked_idx],name_to_QID,QID_to_eid,eid_to_embeddings,tokenizer,device=device)\n",
    "    # Predict all tokens\n",
    "        with torch.no_grad():\n",
    "            predictions_custom = model_custom(tokens_tensor, ents_tensor, ent_mask, segments_tensors)\n",
    "            predictions_original =  model_original(tokens_tensor, ents_tensor, ent_mask, segments_tensors)\n",
    "\n",
    "            # confirm we were able to predict 'henson'\n",
    "            predicted_index_custom = torch.argmax(predictions_custom[0, masked_idx]).item()\n",
    "            predicted_token_custom = tokenizer.convert_ids_to_tokens([predicted_index_custom])[0]\n",
    "\n",
    "            print(f'The predicted token is: {predicted_token_custom}')\n",
    "            print(f'The predictions between custom and original are equal: {predictions_custom.eq(predictions_original).all()}')\n",
    "\n",
    "#Load pre-trained model tokenizer (vocabulary)\n",
    "#Special tokenizer for text and entities\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_DIR)\n",
    "\n",
    "#Eg: 'Northern Ireland': 'Q26'\n",
    "name_to_QID = load_name_to_QID(NAME_TO_QID_FILE)\n",
    "#Eg: {'Q11456633': 4525438, 'Q8863973': 1628631}\n",
    "QID_to_eid = load_QID_to_eid(QID_TO_EID_FILE)\n",
    "\n",
    "eid_to_embeddings = torch.load(EID_TO_VEC_FILE)\n",
    "#Creats a dictionnary of entity index->embeddings\n",
    "eid_to_embeddings = Embedding.from_pretrained(eid_to_embeddings)\n",
    "\n",
    "text_a = \"Who was Jim Henson ? \"\n",
    "text_b = \"Jim Henson was a puppeteer .\"\n",
    "#masked_indices = [8]\n",
    "\n",
    "#tokens_tensor,ents_tensor,ent_mask,segments_tensors = process_sentences(text_a,text_b,masked_indices,name_to_QID,QID_to_eid,tokenizer)\n",
    "\n",
    "eval_sentence(text_a,text_b,model_original,model_custom,tokenizer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5dd83c0049ca90416432c7bf31543fa4a43c6d1cbcb766d79372bfcb46406b7f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
